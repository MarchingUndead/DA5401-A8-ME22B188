{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decee58c",
   "metadata": {},
   "source": [
    "# DA5401 — Assignment 8: Ensemble Learning for Bike Sharing\n",
    "**Author:** (Your Name)  \n",
    "**Course:** DA5401 — Mathematical Foundations of Data Science (Assignment A8)  \n",
    "**Dataset:** UCI Bike Sharing Demand (via ucimlrepo, ID=275)\n",
    "\n",
    "**Contents**\n",
    "1. Data loading & preprocessing (ucimlrepo)  \n",
    "2. Baseline models (Decision Tree, Linear Regression)  \n",
    "3. Bagging  \n",
    "4. Gradient Boosting  \n",
    "5. Stacking  \n",
    "6. Comparative RMSE results & plots  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5572945",
   "metadata": {},
   "source": [
    "## Part A — Data Loading and Preprocessing (ucimlrepo)\n",
    "We fetch the dataset directly using `ucimlrepo` and perform preprocessing:\n",
    "- Drop unnecessary columns  \n",
    "- Encode categorical variables  \n",
    "- Train/test split  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27135bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Data loading and preprocessing using ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fetch dataset directly from UCI ML Repo (ID = 275, Bike Sharing)\n",
    "bike_sharing = fetch_ucirepo(id=275)\n",
    "\n",
    "# Extract features (X) and targets (y)\n",
    "X_full = bike_sharing.data.features.copy()\n",
    "y_full = bike_sharing.data.targets.copy()\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df = pd.concat([X_full, y_full], axis=1)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=['instant','dteday','casual','registered'], errors='ignore')\n",
    "\n",
    "# Convert categorical columns\n",
    "cat_cols = ['season','weathersit','mnth','hr','weekday']\n",
    "for c in cat_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype('category')\n",
    "\n",
    "# One-hot encoding\n",
    "df_enc = pd.get_dummies(df, columns=[c for c in cat_cols if c in df.columns], drop_first=True)\n",
    "\n",
    "# Target and features\n",
    "if 'cnt' in df_enc.columns:\n",
    "    y = df_enc['cnt']\n",
    "    X = df_enc.drop(columns=['cnt'])\n",
    "else:\n",
    "    y = df_enc.iloc[:, -1]\n",
    "    X = df_enc.iloc[:, :-1]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e787a",
   "metadata": {},
   "source": [
    "### Baseline Models — Decision Tree and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import sqrt\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "rmse_dt = sqrt(mean_squared_error(y_test, dt.predict(X_test)))\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "rmse_lr = sqrt(mean_squared_error(y_test, lr.predict(X_test)))\n",
    "\n",
    "print(f\"Decision Tree RMSE: {rmse_dt:.4f}\")\n",
    "print(f\"Linear Regression RMSE: {rmse_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da40c50",
   "metadata": {},
   "source": [
    "## Part B — Bagging and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef620ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Bagging\n",
    "bag = BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=6), n_estimators=50, random_state=42, n_jobs=-1)\n",
    "bag.fit(X_train, y_train)\n",
    "rmse_bag = sqrt(mean_squared_error(y_test, bag.predict(X_test)))\n",
    "\n",
    "# Gradient Boosting\n",
    "gbr = GradientBoostingRegressor(random_state=42, n_estimators=200, learning_rate=0.1, max_depth=3)\n",
    "gbr.fit(X_train, y_train)\n",
    "rmse_gbr = sqrt(mean_squared_error(y_test, gbr.predict(X_test)))\n",
    "\n",
    "print(f\"Bagging RMSE: {rmse_bag:.4f}\")\n",
    "print(f\"Gradient Boosting RMSE: {rmse_gbr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a73a60",
   "metadata": {},
   "source": [
    "## Part C — Stacking Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46081874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "bag_for_stack = BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=6), n_estimators=50, random_state=42, n_jobs=-1)\n",
    "gbr_for_stack = GradientBoostingRegressor(random_state=42, n_estimators=200, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "estimators = [('knn', knn), ('bag', bag_for_stack), ('gbr', gbr_for_stack)]\n",
    "meta_learner = Ridge(alpha=1.0)\n",
    "\n",
    "stack = StackingRegressor(estimators=estimators, final_estimator=meta_learner, n_jobs=-1)\n",
    "stack.fit(X_train, y_train)\n",
    "rmse_stack = sqrt(mean_squared_error(y_test, stack.predict(X_test)))\n",
    "print(f\"Stacking RMSE: {rmse_stack:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a112d0",
   "metadata": {},
   "source": [
    "## Part D — Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Linear Regression', 'Bagging', 'Gradient Boosting', 'Stacking'],\n",
    "    'RMSE': [rmse_dt, rmse_lr, rmse_bag, rmse_gbr, rmse_stack]\n",
    "}).sort_values('RMSE')\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Plot RMSE comparison\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(results['Model'], results['RMSE'])\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.title('RMSE Comparison')\n",
    "plt.ylabel('RMSE')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05440009",
   "metadata": {},
   "source": [
    "### Conclusion — Bias-Variance and Ensemble Insights"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
